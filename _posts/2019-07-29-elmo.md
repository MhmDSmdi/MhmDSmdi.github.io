---
title: 'Contextualized word embedding: ELMo'
date: 2019-07-29
permalink: /posts/2019/07/elmo/
tags:
  - NLP
  - DeepLearning
---


ELMo is a great Language Model for contextualized word embedding, in this post I'd explain how it works.


## Introduction
Every language model ideally should model: 
1. Essentioal properties of word use (syntax or semantics) 
2. How these uses change across linguistic context for example model polysemy
Also you can read more about Language Model [here](https://en.wikipedia.org/wiki/Language_model).

ELMo (**E**mbedding from **L**anguage **Mo**del) is a word representation that can deal with these challenges properly. ELMo uses a two layer bidirectional LSTM because of that for every word, the model learns a linear combination of vectors stacked above each input. If you are not familiar with LSTM, check this awesome [tutorial](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).

### How ELMo works?
Two layer bi-LM is like as follows: 

![bi ln](https://raw.githubusercontent.com/MhmDSmdi/mhmdsmdi.github.io/master/images/bi_LN.png)

The first bi-LM layer captures syntactic informations while the second layer obtains context dependent aspects of word meaning. Also these layers are connected to each other and makes up our model.

![GIF](https://raw.githubusercontent.com/MhmDSmdi/mhmdsmdi.github.io/master/images/bi_LN_vectors.png)

At first, each token is converted to appropriate representation using character embedding which is a complex, you can find it [here](https://arxiv.org/pdf/1508.06615.pdf).
This conversion has several benefits, for example it allows the model to form a correct representation for out-of-vocabulary words.
After that, we feed it into the bi-LM and get their outputs.

Let's continue with an example, assume that we are looking for "good" word. The model combines the outputs of 2 layer bi-LM and raw vector of "good" into new vector like below:

![IMG](https://raw.githubusercontent.com/MhmDSmdi/mhmdsmdi.github.io/master/images/elmo.png)

As noted in the paper, this new vector calculated as follow:

`ELMo('good') =γ⋅(s0⋅x('good') + s1⋅h1('good') + s2⋅h2('good'))`

Which Si represents softmax-normalized weights on the hidden representations from the language model and γ represents a specific scaling factor.

### How ELMo is trained?
the ELMo language model is trained on a huge dataset and the final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer.
As a result, the biLM provides three layers of representations for each input token, including those outside the training set due to the purely character input. In contrast, traditional word embedding methods (Word2vec or Glove) only provide one layer of representation for tokens in a fixed vocabulary.

Also representation vector of a word is not uniqe and it might be different in a distinct sentence. For instance, assume these two sentences :
1. I put my books in the bag.
2. She put her book in my bag yesterday.

As is clear, the verb "put" has different meaning in these sentences(the first is a preesnt tense and the second is a past tense). But traditional word embeddings consider same vector representation for "put". In contrast ELMo gives distinct vectors for "put" as a output.

## Conclusion
ELMo is a high-quality deep context-dependent language model for word representations from biLMs, and shown large improvements when applying ELMo to a broad range of NLP tasks. 

## References
* [Deep contextualized word representations](https://arxiv.org/abs/1802.05365)
