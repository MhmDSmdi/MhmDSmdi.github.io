---
title: 'Contextualized word embedding: ELMo'
date: 2019-07-29
permalink: /posts/2019/07/elmo/
tags:
  - NLP
  - DeepLearning
---


## Introduction
Every language model ideally should model: 
1. essentioal properties of word use (syntax or semantics) 
2. how these uses change across linguistic context for example model polysemy
Also you can read more about Language Model [here](https://en.wikipedia.org/wiki/Language_model).

ELMo (**E**mbedding from **L**anguage **Mo**del) is a word representation that can deal with these challenges properly. ELMo uses a two layer bidirectional LSTM because of that for every word, the model learns a linear combination of vectors stacked above each input. If you are not familiar with LSTM, check this awesome [tutorial](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).

### How ELMo works?
Two layer bi-LM is like as follows: 
![IMG]()
The first bi-LM layer captures syntactic informations while the second layer obtains context dependent aspects of word meaning. Also these layers are connected to each other and makes up our model.
![GIF]()
At first, each token is converted to appropriate representation using character embedding which is a complex, you can find it [here](https://arxiv.org/pdf/1508.06615.pdf).
This conversion has several benefits, for example it allows the model to form a correct representation for out-of-vocabulary words.
After that, we feed it into the bi-LM and get their outputs.

Let's continue with an example, assume that we are looking for "good" word. The model combines the outputs of 2 layer bi-LM and raw vector of "good" into new vector like below:
![IMG]()

## Conclusion

## References
